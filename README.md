# tech-ai-assist
AI assistent to analyse requirements document
Отлично, давайте разберем ваш план по созданию базы данных ГОСТ/СНиП и использованию AI для анализа технических заданий (ТЗ), фокусируясь на бесплатных ресурсах.

## Задача: Создать БД стандартов (ГОСТ, СНиП) и инструмент для проверки ТЗ на соответствие этим стандартам.
Доступные Вычислительные мощности:
    * **Google Colaboratory (Colab):** Предоставляет бесплатный доступ к средам Jupyter Notebook с GPU и TPU (с ограничениями по времени и ресурсам). Идеально для экспериментов, разработки, обучения (небольших моделей или дообучения) и запуска анализа.
    * **Kaggle Kernels:** Аналогично Colab, бесплатные ноутбуки с GPU.
    * **Локальный компьютер:** Вариант, если у вас есть достаточно мощный ПК (особенно с современной видеокартой NVIDIA), потом будем запускать многие модели локально.
2.  **База данных:**
    * Начать с комбинации **SQLite** (для метаданных) и **ChromaDB/FAISS** (для хранения векторов текстов и семантического поиска).

**Выбор нейронной сети (модели):**
1.  **Для семантического поиска (Embedding Models):** Эти модели преобразуют текст в векторы (эмбеддинги), где близкие по смыслу тексты имеют близкие векторы.
    * **Sentence Transformers:** Библиотека и набор моделей, оптимизированных для этого. Модели, поддерживающие русский или мультиязычные:
        * `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` (хорошая мультиязычная модель)
        * `sentence-transformers/distiluse-base-multilingual-cased-v1/v2` (другой вариант)
        * Модели от российских команд, если доступны и имеют подходящую лицензию (искать по "russian sentence transformer" или смотреть модели от Sber, Tinkoff, Yandex на Hugging Face).
    * **Важно:** Выбрать *одну* модель и использовать *ее же* для векторизации и ГОСТов, и ТЗ.
2.  **Для более глубокого анализа (потенциально, на следующих этапах):**
    * **BERT-подобные модели для русского языка:** `DeepPavlov/rubert-base-cased`, `sberbank-ai/ruBERT-base` и их вариации. Можно дообучить на задачи Question Answering (Ответы на вопросы) или Natural Language Inference (Определение логического следствия/противоречия), чтобы проверять, противоречит ли пункт ТЗ найденному пункту ГОСТа. *Это сложнее и требует больше ресурсов.*

**Загрузка ГОСТ и СНиП в базу данных:**
1.  **Шаг А: Сбор данных:** (TXT, DOCX, HTML, возможно PDF с текстовым слоем). Возможные пути:
    * Парсинг открытых государственных порталов (сложно, требует навыков веб-скрейпинга).
    * Поиск в открытых библиотеках, университетских репозиториях.
    * Ручной набор или использование OCR (распознавание текста) для сканов (требует вычитки и коррекции ошибок).
    * *Предположим, вы раздобыли тексты.*
2.  **Шаг Б: Предобработка текста:**
    * Очистка от "мусора": колонтитулы, номера страниц, артефакты сканирования/OCR.
    * Удаление ненужной разметки, приведение к единому формату (например, чистый текст).
3.  **Шаг В: Разбиение на чанки (Chunks):** Нельзя векторизовать весь ГОСТ целиком. Нужно разбить его на осмысленные, относительно небольшие фрагменты (чанки):
    * По пунктам/подпунктам стандарта.
    * По параграфам.
    * По смысловым блокам.
    * По отдельным требованиям
    * Размер чанка влияет на качество поиска (слишком большие – размытый смысл, слишком маленькие – потеря контекста). Экспериментируйте (например, 100-500 слов).
4.  **Шаг Г: Извлечение метаданных:** Для каждого чанка: номер ГОСТ/СНиП, название документа, номер раздела/пункта, возможно, дата документа.
5.  **Шаг Д: Создание эмбеддингов и загрузка в векторную БД:**
    * Выбираем модель `paraphrase-multilingual-mpnet-base-v2` (из рекомендованных выше).
    * Пишем скрипт (на Python в Colab):
        * Читает каждый чанк текста.
        * Подает его в модель эмбеддингов, получает вектор.
        * Загружает в векторную БД (например, ChromaDB) сам текст чанка, его вектор и метаданные (ID ГОСТа, номер пункта и т.д.).
    * Параллельно можно загружать метаданные в SQLite для удобных запросов по атрибутам (найти все чанки из ГОСТа ХХХ).

**План реализации:**
1. Создадим новый ноутбук в Colab



**Шаг 0: Подготовка окружения**
* Определимся с платформой (Google Colab – рекомендуется).
* Создадим новый ноутбук в Colab.
* Установим необходимые библиотеки: `transformers`, `sentence-transformers`, `datasets`, `chromadb`, `sqlite3`, `nltk` (для разбиения текста).
    ```python
    !pip install -q transformers sentence-transformers datasets chromadb sqlite3 nltk
    ```
* Настроим доступ к Google Drive (если данные ГОСТов будут храниться там).

**Шаг 1: Сбор и подготовка данных ГОСТ/СНиП (Ваша задача)**
* **Ваша задача:** Найти и собрать тексты хотя бы нескольких ГОСТов/СНиПов в текстовом формате (например, TXT). Поместить их в папку (локально или на Google Drive).
* **Результат:** Папка с текстовыми файлами стандартов.

**Шаг 2: Написание скрипта предобработки и чанкинга**
* **Наша задача:** Написать Python-функции для:
    * Чтения текстовых файлов.
    * Базовой очистки текста.
    * Разбиения текста на чанки (например, по параграфам или с использованием `nltk.sent_tokenize` и объединением предложений до нужной длины).
    * Извлечения метаданных (например, имя файла как ID документа).
* **Результат:** Список чанков, где каждый элемент – это словарь `{'text': 'текст чанка...', 'metadata': {'doc_id': 'GOST_XXX', 'chunk_id': 1}}`.

**Шаг 3: Настройка баз данных**
* **Наша задача:** Написать код для инициализации:
    * SQLite базы данных (создание таблицы для метаданных).
    * ChromaDB клиента (создание коллекции для хранения векторов).
* **Результат:** Готовые к наполнению экземпляры БД.

**Шаг 4: Выбор модели и создание эмбеддингов**
* **Наша задача:**
    * Выбрать конкретную модель Sentence Transformer из Hugging Face Hub (например, `paraphrase-multilingual-mpnet-base-v2`).
    * Загрузить модель с помощью библиотеки `sentence-transformers`.
    * Написать цикл, который проходит по списку чанков из Шага 2.
    * Для каждого чанка генерирует вектор эмбеддинга с помощью загруженной модели.
* **Результат:** Список чанков с добавленными векторами.

**Шаг 5: Загрузка данных в БД**
* **Наша задача:** Написать код для:
    * Загрузки метаданных и текста (или ID чанка) в SQLite.
    * Загрузки текста (или ID), вектора и метаданных в ChromaDB.
* **Результат:** Наполненная база данных ГОСТ/СНиП, готовая к поиску.

**Шаг 6: Подготовка функции для анализа ТЗ**
* **Наша задача:** Написать функцию, которая:
    * Принимает на вход текст требования из ТЗ.
    * Генерирует эмбеддинг для этого требования (используя ту же модель из Шага 4).
    * Выполняет поиск в ChromaDB по вектору, чтобы найти N самых похожих чанков из ГОСТ/СНиП.
    * Возвращает найденные чанки (текст + метаданные).
* **Результат:** Функция для поиска релевантных норм.

**Шаг 7: Создание интерфейса "AI-ассистента" (Минимальный вариант)**
* **Наша задача:** Написать простой код, который:
    * Запрашивает у пользователя пункт ТЗ.
    * Использует функцию из Шага 6 для поиска релевантных норм.
    * Выводит на экран пункт ТЗ и найденные пункты ГОСТ/СНиП для сравнения **человеком-экспертом**.
* **Результат:** Прототип ассистента.

**Шаг 8: Тестирование и итерация**
* **Наша задача (и ваша):** Протестировать работу на реальных примерах ТЗ и ГОСТ. Оценить качество поиска. Собрать обратную связь от потенциального эксперта.
* **Возможные улучшения:** Подбор размера чанка, выбор другой модели эмбеддингов, улучшение очистки текста, добавление фильтрации по метаданным при поиске.

**Начнем с Шага 0 и Шага 1?** Подтвердите, что вы готовы начать поиск и подготовку текстовых файлов ГОСТ/СНиП, и мы сможем перейти к написанию кода для Шага 2 в Google Colab.
