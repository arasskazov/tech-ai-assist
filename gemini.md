Отлично! Шаг 0 и Шаг 1 выполнены – данные на месте.

**Следующий шаг (Шаг 2): Предобработка данных и разбиение на чанки (фрагменты).**

На этом шаге наша цель – написать код, который сможет:
1.  Подключиться к вашему Google Drive из Google Colab.
2.  Найти и прочитать текстовые файлы ГОСТ/СНиП из указанной вами папки.
3.  Выполнить базовую очистку текста (если потребуется).
4.  Разбить текст каждого документа на осмысленные фрагменты (чанки).
5.  Присвоить каждому чанку метаданные (из какого документа он взят, его порядковый номер).
6.  Сохранить результат в структурированном виде (список словарей) для дальнейшей обработки.

Давайте сделаем это по порядку в вашем Google Colab ноутбуке.

**1. Подключение к Google Drive:**

Выполните этот код в ячейке Colab. Вам нужно будет перейти по ссылке, разрешить доступ и скопировать код авторизации обратно в ячейку.

```python
from google.colab import drive
drive.mount('/content/drive')
```

**2. Указание пути к папке и проверка файлов:**

Теперь вам нужно указать *точный путь* к папке с вашими файлами ГОСТ/СНиП внутри Google Drive. **Ссылка, которую вы предоставили, работает в веб-интерфейсе, но в коде нужен путь к файловой системе.** Он обычно начинается с `/content/drive/MyDrive/`.

**Замените** `'ЗАМЕНИТЕ_НА_ВАШ_ПУТЬ'` на правильный путь к вашей папке в коде ниже и выполните его.

```python
import os

# --- !!! ВАЖНО: ЗАМЕНИТЕ ЭТОТ ПУТЬ !!! ---
# Укажите здесь правильный путь к папке с ГОСТами на вашем Google Диске
# Пример: '/content/drive/MyDrive/Colab Notebooks/GOST_Data'
data_folder_path = 'ЗАМЕНИТЕ_НА_ВАШ_ПУТЬ/0AG24LjW9Y6L4Uk9PVA' # <--- ИЗМЕНИТЕ ЭТОТ ПУТЬ!!!

try:
    # Проверяем существование папки
    if not os.path.isdir(data_folder_path):
        print(f"ОШИБКА: Папка не найдена по пути: '{data_folder_path}'")
        print("Пожалуйста, проверьте правильность пути в переменной data_folder_path.")
        file_list = []
    else:
        # Получаем список файлов (только файлы, исключая подпапки)
        file_list = [f for f in os.listdir(data_folder_path) if os.path.isfile(os.path.join(data_folder_path, f))]
        if not file_list:
            print(f"В папке '{data_folder_path}' не найдено файлов.")
            print("Убедитесь, что файлы загружены и путь указан верно.")
        else:
            print(f"Найдены файлы ({len(file_list)} шт.):")
            for fname in file_list:
                print(f"- {fname}")

except Exception as e:
    print(f"Произошла ошибка при доступе к папке или файлам: {e}")
    file_list = []

# Переменная file_list теперь содержит список имен файлов (если найдены)
```

**Убедитесь, что код успешно находит ваши файлы.** Если возникает ошибка "Папка не найдена", дважды проверьте путь.

**3. Чтение и просмотр содержимого одного файла:**

Давайте прочитаем первый найденный файл, чтобы посмотреть на его содержимое и определиться со стратегией чанкинга. Код попытается прочитать файл в кодировке UTF-8, а если не получится – в CP1251 (частая кодировка для старых русских текстов).

```python
raw_text = None # Переменная для хранения текста

if file_list: # Продолжаем, только если файлы были найдены на предыдущем шаге
    example_file_name = file_list[0] # Берем первый файл для примера
    example_file_path = os.path.join(data_folder_path, example_file_name)
    print(f"\n--- Чтение файла: {example_file_name} ---")

    try:
        with open(example_file_path, 'r', encoding='utf-8') as f:
            raw_text = f.read()
        print(f"Файл успешно прочитан в кодировке UTF-8.")
    except UnicodeDecodeError:
        print("Не удалось прочитать в UTF-8, пробую CP1251...")
        try:
            with open(example_file_path, 'r', encoding='cp1251') as f:
                raw_text = f.read()
            print(f"Файл успешно прочитан в кодировке CP1251.")
        except Exception as e_cp1251:
            print(f"Не удалось прочитать файл и в CP1251. Ошибка: {e_cp1251}")
    except Exception as e_utf8:
        print(f"Не удалось прочитать файл в UTF-8. Ошибка: {e_utf8}")

    if raw_text:
        print(f"Длина текста: {len(raw_text)} символов.")
        print("\n--- Начало файла (первые 500 символов): ---")
        print(raw_text[:500])
        print("--------------------------------------------")
    else:
        print("Текст файла не был прочитан.")
else:
    print("\nСписок файлов пуст. Не могу продолжить чтение.")

```

Посмотрите на выведенный фрагмент текста. Как он структурирован? Есть ли явное разделение на параграфы (пустые строки между ними)? Есть ли нумерация пунктов? Это поможет выбрать способ разбиения на чанки.

**4. Разбиение текста на чанки (Пример: по параграфам):**

Самый простой способ – разбить текст по пустым строкам (параграфам). Давайте попробуем его. Мы также отфильтруем слишком короткие фрагменты.

```python
import re

def chunk_by_paragraphs(text, min_length=50):
    """
    Разбивает текст на параграфы (по двойным или более переводам строки)
    и фильтрует слишком короткие или пустые фрагменты.
    """
    if not text:
        return []
    # Используем регулярное выражение для разделения по двум или более переводам строки
    paragraphs = re.split(r'\n\s*\n+', text)
    # Очищаем от лишних пробелов в начале/конце и фильтруем
    chunks = [p.strip() for p in paragraphs if len(p.strip()) >= min_length]
    return chunks

chunks_list = [] # Переменная для хранения чанков
if raw_text: # Продолжаем, если текст был успешно прочитан
    print("\n--- Разбиение текста на чанки (по параграфам) ---")
    chunks_list = chunk_by_paragraphs(raw_text, min_length=100) # Мин. длина чанка - 100 символов
    print(f"Получено чанков: {len(chunks_list)}")
    if chunks_list:
        print("\n--- Пример первого чанка: ---")
        print(chunks_list[0])
        print("-----------------------------")
    else:
        print("Не удалось создать чанки. Возможно, текст имеет другую структуру или слишком короткий.")
else:
    print("\nНе могу разбить текст на чанки, так как он не был прочитан.")

```
*Примечание: Если разбивка по параграфам не подходит (например, текст идет сплошняком или разделен только одинарными переводами строк), нам нужно будет использовать другую стратегию (например, разбивку по предложениям с помощью `nltk` и их объединение в чанки).*

**5. Структурирование данных:**

Теперь преобразуем полученный список текстовых чанков в нужный нам формат: список словарей, где каждый словарь содержит текст чанка и его метаданные.

```python
def structure_chunks(chunks, doc_id):
    """Преобразует список текстов чанков в список словарей с метаданными."""
    structured_data = []
    if not chunks:
        return structured_data
    base_doc_id = os.path.splitext(doc_id)[0] # Используем имя файла без расширения
    for i, chunk_text in enumerate(chunks):
        structured_data.append({
            # 'id': f"{base_doc_id}_chunk_{i}", # Уникальный ID чанка (добавим позже при загрузке в БД)
            'text': chunk_text,
            'metadata': {
                'doc_name': doc_id, # Полное имя файла источника
                'doc_id': base_doc_id, # Имя файла без расширения
                'chunk_num': i # Порядковый номер чанка в документе
                # Сюда можно будет добавить больше метаданных позже (номер пункта и т.д.)
            }
        })
    return structured_data

all_processed_data = [] # Здесь будем хранить обработанные данные всех файлов
if chunks_list: # Продолжаем, если чанки были созданы
    print("\n--- Структурирование данных ---")
    # В этом примере используем имя первого файла, прочитанного ранее
    processed_data_single_file = structure_chunks(chunks_list, example_file_name)
    print(f"Количество структурированных элементов: {len(processed_data_single_file)}")
    if processed_data_single_file:
        print("\n--- Пример первого структурированного элемента: ---")
        # Используем pprint для красивого вывода словаря
        import pprint
        pprint.pprint(processed_data_single_file[0])
        print("----------------------------------------------------")
        # Сохраняем результат (пока только для одного файла)
        all_processed_data.extend(processed_data_single_file)
    else:
        print("Не удалось создать структурированные данные.")
else:
    print("\nНе могу структурировать данные, так как чанки не были созданы.")

# На следующих этапах мы будем использовать переменную all_processed_data
# В реальном коде здесь был бы цикл по всем файлам из file_list
# и добавление их обработанных данных в all_processed_data
```

**Ваша задача сейчас:**
1.  Выполнить код из пунктов 1-5 в вашем Colab ноутбуке.
2.  **Обязательно исправьте путь** в переменной `data_folder_path` в пункте 2.
3.  Проанализировать вывод: корректно ли находятся файлы? Успешно ли читается текст? Подходит ли разбивка на чанки по параграфам для ваших файлов (посмотрите на пример первого чанка)? Создается ли структурированный список словарей?

Как только вы успешно выполните эти шаги для одного файла и убедитесь, что результат вас устраивает (особенно способ разбивки на чанки), мы сможем перейти к **Шагу 3: Настройка баз данных (SQLite и ChromaDB)**.

Сообщите, когда будете готовы двигаться дальше или если возникнут проблемы на этом этапе!
